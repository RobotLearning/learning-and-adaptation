\documentclass[10pt,a4paper]{article}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\author{Okan Ko\c c}
\title{Contextual Gaussian Process Bandit Optimization for learning in dynamical systems}
\begin{document}
\maketitle

%Problem Statement

Systems that work in a repetitive manner, such as robot arm manipulators and chemical batch processes, use Iterative Learning Control (ILC) to iteratively improve the performance over a given repeated task or trajectory. The feed-forward control signal is modified in each iteration to reduce the error or the deviation from the given reference trajectory. A good analogy is a basketball player shooting a free throw from a fixed position: during each shot the basketball player can observe the trajectory of the ball and alter the shooting motion in the next attempt \cite{Survey}. 

The limitation with ILC is that it assumes the task or the trajectory to be fixed (constant) over iterations. While this is a reasonable assumption to make in some repeated tasks, ILC cannot handle the cases when the trajectory is modified or changing over time, and the learning controller must start learning from scratch. We believe that a significant amount of knowledge can be transferred even between cases where the reference trajectories are not the same. A basketball player does not have to learn the free throw motion from scratch each time he finds himself in a slightly different position. 

%RoboEarth

We call these different cases or reference trajectories \textit{contexts}. Context can change in a given task and it is the responsibility of the autonomous agent or the learning controller to adapt to different contexts. The ability to transfer knowledge even under different contexts will significantly improve the performance of RoboEarth\cite{RoboEarth}, a robot specific knowledge repository where robots share and learn from each others' experience. As an example, consider a robot operating in a private home in Eindhoven learning to pour tea from a teapot into a teacup, and over time perfecting the motion. The learned pouring motion can be uploaded to RoboEarth, marked with the hardware-specifics of the particular robot as well as the size and shape of the teapot/teacup as context. Another robot in Zaragoza\footnote{University of Zaragoza and Eindhoven University of Technology are partners with ETH Zrich in the RoboEarth project. For the list of all partners and other details visit http://www.roboearth.org/} with slightly different hardware and holding a different teapot, can download the stored motion as a prior and adapt it to its particular context, thereby eliminating the need to learn the motion from scratch. 

%Tools to be used
%To accomplish this task we have used - use in the thesis
For this task, we plan to use Gaussian Processes for estimation and multi-armed bandits for optimization \& knowledge transfer between different contexts.
\begin{scriptsize}
\begin{footnotesize}
\end{footnotesize}
\end{scriptsize}

%Gaussian processes
A \textit{Gaussian Process} (GP) is a collection of dependent random variables, any finite number of which have a joint Gaussian distribution \cite{GPbook}. 
Gaussian processes (known in the statistics literature as \textit{kriging}) have been used in the statistics community for a long time, especially in geostatistics. GP is currently applied extensively in machine learning, mostly in Bayesian supervised learning, in both regression and classification.

Gaussian processes, specified by a mean function $\mu(x)$ and a covariance (or kernel) function $k(x,x')$, can be seen as a prior over the space of functions. For $y_i = f(x_i) + \epsilon_i$, $\epsilon_i \sim \mathcal{N}(0,\sigma^2)$ with noisy observations $\mathbf{y}_{N} = \{y_1,\ldots,y_N\}$ at sample points $A_N = \{x_1,\ldots,x_N\}$, the posterior over $f$ is again a Gaussian Process distribution with mean $\mu_N{(x)}$ and covariance $k_N(x,x')$ given by the following simple analytic equations:
\begin{eqnarray}
\mu_N{(x)} &=& \mathbf{k}_N(x)^{\mathrm{T}}[\mathbf{K}_N + \sigma^{2}\mathbf{I}]^{-1}\mathbf{y}_N, \label{gpUpdate_mu}\\ 
k_N(x,x') &=& k(x,x') - \mathbf{k}_N(x)^{\mathrm{T}}[\mathbf{K}_N + \sigma^{2}\mathbf{I}]^{-1} \mathbf{k}_N(x'), 
\label{gpUpdate_sigma}
\end{eqnarray}
where $\mathbf{k}_N(x) = [k(x_1,x),\ldots,k(x_T,x)]^\mathrm{T}$ and  $\mathbf{K}_N = [k(x,x')]_{x,x' \in A_N} \succ 0.$

A Gaussian process is a nonparametric method where overfitting encountered typically in regression with finite basis functions does not occur. The priors specified by the mean and variance functions have a lot of flexibility and can be further optimized for training data by specifying \textit{hyperparameters}. There are many prior functions (including composites) to choose from, and their hyperparameters can be optimized by using likelihood functions for training data.
%\footnote{Squared exponential (i.e. unnormalized Gaussian) is one of the common covariance functions used for regression.}.
The likelihood function specifies the probability of the noisy training data $\mathbf{y}$ given the sample points and latent function, i.e. the GP and the hyperparameters. 

%mention information gain in 2nd equation 
Gaussian Processes when used in estimation, have the advantage of capturing the entire distribution over future values of the function instead of merely their expectation \cite{Rasmussen2}, but unlike Kalman filters require $O(N^3)$ complexity due to the prohibitive matrix inversion in (\ref{gpUpdate_mu}) and (\ref{gpUpdate_sigma}). 
Several approximation methods such as the Bayesian Committee Machine are mentioned in the literature \cite{GPbook} to overcome the problem of growing $N$ especially in big datasets.
%
GP has been recently applied in control, for example see \cite{Blimp} for a hybrid-GP
% \footnote{classical dynamics + added GP} % too much details
approach combined with reinforcement learning to control an autonomous blimp. 

%Contextual Bandits
Multi-armed bandit problems is the basic setting in decision theory and reinforcement learning to optimally balance \textit{exploration}  and \textit{exploitation}. In the context of stochastic function optimization, in order to find the global maximum of an unknown noisy function, it must be \textit{explored} by sampling at promising points and must be \textit{exploited} at the current maximum values. Using upper confidence bounds (UCB) is a particularly simple and intuitive heuristic to balance the exploration/exploitation trade off and works by keeping track of upper confidence bounds of the sample points \cite{Krause1}.  When applied to dynamical systems, 
% This is the only place we mention [dynamical systems] which is part of the topic. Let's keep it that way.
the sample points are the control signals and the function to be minimized is the cost function. Although the cost function w.r.t. the system output is well defined, the cost function w.r.t. the control signal is not completely known due to modelling errors and unknown external disturbances. Therefore, multi-armed bandits can be used to track the global minimum of this cost function and is expected to yield  performance on par with existing ILC methods while providing a framework that can be extended for knowledge transfer.
%yields better performance, defined in terms of the error signal in a particular iteration. 
%defined in terms of the output of the dynamical system. Since, typically we know only the dynamical model from the first principles, the cost function in terms of input is not completely known and optimizing this cost function with inputs can benefit from using multi-armed bandits.

In addition to the controller input, contexts such as the current internal state of the system or the current environment state can be parameterized in the cost function. %and learned from training data using likelihood functions.
Learning over the joint space of inputs and contexts enables the knowledge transfer between different learning controllers with different reference trajectories.
%If the function to be optimized is changing over time, then the optimal action will be to keep exploring. The function in this case can change \textit{states} or reveal different \textit{contexts}. 
With the addition of context, bandit problems transform into a full-scale reinforcement learning problem \cite{sutton98a}. In reinforcement learning literature, tracking the global optimum is known as minimizing the cumulative regret
%~\footnote{Equivalently, maximizing the cumulative reward.}  
$R_{T} = \Sigma_{t = 1}^{T}r_{t} $ . A desired feature of a (reinforcement) learning algorithm is to have sublinear regret. Krause et al. in \cite{Krause2}, discuss such a learning algorithm (called CGP-UCB) which maintains a GP-estimate of the function to be optimized: 
\begin{equation}
u_{t} = \operatorname*{arg\,max}_{u \in U}\mu_{t-1}(u,c_t) + \beta_{t}^{1/2}\sigma_{t-1}(u,c_t) \nonumber
\end{equation}

where $\mu_{t-1}$ and $\sigma_{t-1}$ are the posterior mean and standard deviation of the GP over the joint input-context
space $U \times C$, conditioned on the observations $(u_1,c_1; y_1),\ldots,(u_{t-1},c_{t-1}; y_{t-1})$ and $\beta_{t}$ is the confidence parameter that scales exploration vs. exploitation for the particular problem. Thus, when presented with context $c_t$, this algorithm uses GP-posterior inference in (\ref{gpUpdate_mu}) and (\ref{gpUpdate_sigma}) to predict mean and variance for each possible action $u$, conditioned on all past observations.
%The sublinear regret
%~\footnote{Regret bounds are always with high probability since they're derived from Hoeffding's inequality.} 
%of CGP-UCB depends on this parameter as well as the maximum information gain. 

%Milestones
We believe that this algorithm, or a variant, can be utilized in the analysis and algorithmic design of knowledge transfer between different contexts. Starting with toy-problems, such as stabilizing the \emph{inverted pendulum}, we plan to show its effectiveness in increasingly complex problems. The next step would be to learn iteratively a more accurate model of a car driving on a given trajectory. By using contextual bandits on such a problem, we expect to improve the system response of the car over different trajectories. We plan to avoid the high-dimensionality of a discretized trajectory by projecting it to a subspace, or by extracting useful features from it. These features will constitute the context-space for the contextual bandit.

%We plan to use and if necessary further develop Rasmussen's Gaussian Processes MATLAB toolbox. Additional software necessary to implement Contextual Bandits and Iterative Learning Control will be integrated with the toolbox. The implemented method will be tested on several (and increasingly complex) system dynamics models and compared with existing methods.
% Too much details

%Mention Angela's research

%include references
\bibliography{proposal}{}
\bibliographystyle{plain}

\end{document}